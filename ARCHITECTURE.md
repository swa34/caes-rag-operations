# CAES RAG System Architecture

## Overview

The CAES Intranet Help Bot uses a sophisticated multi-layer RAG (Retrieval-Augmented Generation) architecture with Redis caching, intelligent query processing, and streaming responses.

## High-Level Architecture Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Query Input                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LAYER 1: CACHE CHECK (Early)                   â”‚
â”‚  server.js:238-336                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Redis Cache Lookup (query hash + session context)         â”‚ â”‚
â”‚  â”‚  â€¢ Check BEFORE reframing/clarification                    â”‚ â”‚
â”‚  â”‚  â€¢ TTL: 30 days                                            â”‚ â”‚
â”‚  â”‚  â€¢ Returns cached response immediately if found            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                 â”‚
                    â–¼                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ CACHE HITâ”‚      â”‚ CACHE MISS   â”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                  â”‚
                  â”‚                  â–¼
                  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    â”‚  LAYER 2: CONVERSATION MEMORY           â”‚
                  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                  â”‚    â”‚  â”‚ PostgreSQL Session Management     â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Load last N messages            â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ 30-min session TTL              â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Automatic cleanup               â”‚  â”‚
                  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                       â”‚
                  â”‚                       â–¼
                  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    â”‚  LAYER 3: QUERY PROCESSING              â”‚
                  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                  â”‚    â”‚  â”‚ Clarification Check                â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Detect ambiguous queries        â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Ask clarifying questions        â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Skip RAG if clarification neededâ”‚  â”‚
                  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚    â”‚               â”‚                          â”‚
                  â”‚    â”‚               â–¼                          â”‚
                  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                  â”‚    â”‚  â”‚ Reframing Engine (if needed)      â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Analyze with conversation       â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Detect follow-up questions      â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Add context from history        â”‚  â”‚
                  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                     â”‚
                  â”‚                     â–¼
                  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    â”‚  LAYER 4: RAG RETRIEVAL PIPELINE        â”‚
                  â”‚    â”‚  (5-Stage Intelligent Retrieval)        â”‚
                  â”‚    â”‚                                          â”‚
                  â”‚    â”‚  Stage 1: Acronym Expansion             â”‚
                  â”‚    â”‚  â€¢ 210 UGA acronyms, 0-2ms lookup      â”‚
                  â”‚    â”‚               â–¼                          â”‚
                  â”‚    â”‚  Stage 2: Metadata Filtering            â”‚
                  â”‚    â”‚  â€¢ Query-aware category detection      â”‚
                  â”‚    â”‚               â–¼                          â”‚
                  â”‚    â”‚  Stage 3: Vector Search + Date Priority â”‚
                  â”‚    â”‚  â€¢ Pinecone vector search              â”‚
                  â”‚    â”‚  â€¢ Recent content prioritization       â”‚
                  â”‚    â”‚               â–¼                          â”‚
                  â”‚    â”‚  Stage 4: Feedback Learning            â”‚
                  â”‚    â”‚  â€¢ Â±10% score adjustment               â”‚
                  â”‚    â”‚               â–¼                          â”‚
                  â”‚    â”‚  Stage 5: LLM Re-ranking (GPT-5-nano)  â”‚
                  â”‚    â”‚  â€¢ Smart triggers (20-30% usage)       â”‚
                  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                     â”‚
                  â”‚                     â–¼
                  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    â”‚  LAYER 5: RESPONSE GENERATION           â”‚
                  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                  â”‚    â”‚  â”‚ GPT-5-mini Response Generation    â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Streaming support enabled       â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ Citation generation             â”‚  â”‚
                  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                     â”‚
                  â”‚                     â–¼
                  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚    â”‚  LAYER 6: RESPONSE CACHING              â”‚
                  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                  â”‚    â”‚  â”‚ Store in cache for future queries â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ PostgreSQL + Redis              â”‚  â”‚
                  â”‚    â”‚  â”‚ â€¢ TTL: 30 days                    â”‚  â”‚
                  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                                        â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚  Stream to Client   â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Features by Layer

### Layer 1: Early Cache Check
**Location**: `server.js:238-336`

**Why Early?** Checking cache BEFORE expensive operations (reframing, embedding, retrieval) provides maximum performance gain.

**Performance**:
- Cache hit: ~50ms (just Redis lookup + DB read)
- Cache miss without early check: ~1800ms (full RAG pipeline)
- **Savings**: 97% faster for cached queries

### Layer 2: Conversation Memory
**Location**: `server.js` (session management)

- PostgreSQL-backed persistent storage
- 30-minute session TTL
- Automatic cleanup of expired sessions
- Multi-instance support

### Layer 3: Query Processing

#### Clarification Mode
**Location**: `server.js:346-414`
**Environment Variable**: `ENABLE_CLARIFICATION_MODE=true`

Detects ambiguous queries and asks for clarification instead of guessing:

```
User: "How do I reset it?"
System: "Could you clarify what you need to reset?
  â€¢ Password
  â€¢ Two-factor authentication
  â€¢ Workday settings"
```

#### Reframing Engine
**Location**: `server.js:368-427`

Handles follow-up questions by adding context from conversation history:

```
User: "What's the deadline?"
Context: Previous question about "travel reimbursement"
Reframed: "What's the deadline for travel reimbursement submission?"
```

### Layer 4: RAG Retrieval Pipeline

#### Stage 1: Acronym Expansion
**Location**: `utils/acronymExpander.js`
- 210 UGA-specific acronyms
- In-memory HashMap (0-2ms lookup)

#### Stage 2: Metadata Filtering
**Location**: `vector-ops/retrieve.js:180-226`
- Query-aware category detection
- 15-25% precision improvement

#### Stage 3: Vector Search with Date Prioritization
**Location**: `vector-ops/retrieve.js:167-232`

**Date-Based Prioritization**:
- Extracts dates from filenames/sources
- When scores are similar (within 0.02), prioritizes recent content
- Ensures policies and procedures are up-to-date

#### Stage 4: Feedback Learning
**Location**: `vector-ops/retrieve.js:234-237`
- User thumbs up: +10% score boost
- User thumbs down: -10% score penalty
- Persistent feedback storage

#### Stage 5: LLM Re-ranking
**Location**: `vector-ops/retrieve.js:36-128`
**Model**: GPT-5-nano

**Smart Triggers** (70-80% cost savings):
- Top 3 scores too similar (< 0.05 spread)
- Temporal queries ("latest", "most recent")
- Comparison queries ("difference", "vs")

**GPT-5 Responses API**:
```javascript
const response = await openai.responses.create({
  model: 'gpt-5-nano',
  input: prompt,
  reasoning: { effort: 'minimal' },
  text: {
    verbosity: 'low',
    format: { type: 'json_object' }
  }
});
```

### Layer 5: Response Generation
**Location**: `server.js:591-629`
**Model**: GPT-5-mini

**GPT-5 Responses API** (optimized for reasoning models):
```javascript
const response = await openai.responses.create({
  model: 'gpt-5-mini',
  instructions: systemPrompt,
  input: userQuery,
  reasoning: { effort: 'minimal' },  // Fast chatbot responses
  text: { verbosity: 'low' }          // Concise answers
});
```

**Streaming Support**: Available via `/chat/stream` endpoint

### Layer 6: Response Caching
**Location**: `server.js:808-840`

Caches successful responses for future use with quality control:
- Feedback quality check before caching
- PostgreSQL + Redis dual storage
- 30-day TTL

## Performance Metrics

### Response Time Breakdown (After Optimizations)

**Cache Hit** (~95% of queries after warm-up):
```
Total: ~50ms
â””â”€â”€ Cache lookup: 50ms (Redis + PostgreSQL)
```

**Cache Miss** (new/unique queries):
```
Total: ~1800ms
â”œâ”€â”€ Conversation load: 50ms
â”œâ”€â”€ Clarification check: 100ms
â”œâ”€â”€ Reframing (if needed): 300ms
â”œâ”€â”€ Acronym expansion: 2ms
â”œâ”€â”€ Embedding: 500ms
â”œâ”€â”€ Vector search: 300ms
â”œâ”€â”€ Feedback adjustment: 10ms
â”œâ”€â”€ Re-ranking (if triggered): 400ms
â”œâ”€â”€ Generation: 500ms (streaming)
â””â”€â”€ Cache storage: 35ms
```

### Cache Hit Rates

```
After warm-up (1 hour of usage):
â”œâ”€â”€ Cache hit rate: 95%+
â”œâ”€â”€ Average response time (hit): 50ms
â”œâ”€â”€ Average response time (miss): 1800ms
â””â”€â”€ Overall average: ~200ms
```

## Infrastructure

### Redis Configuration
**Provider**: Sevalla Redis Add-on ($5/month)

**Features**:
- Automatic retry with exponential backoff
- 5-second connection timeout
- Graceful fallback (system works without Redis)
- Survives container restarts

### PostgreSQL Configuration

**Tables**:
- `conversations` - Session storage and conversation history
- `cached_responses` - Response cache with quality control
- `feedback` - User feedback and learning

### Pinecone Configuration
**Index**: `uga-intranet-index`
- Dimensions: 3072 (OpenAI text-embedding-3-large)
- Metric: cosine similarity
- Pre-initialized at server startup

## Known Issues

### âš ï¸ Duplicate Cache Check

**Problem**: Cache is checked in two places:
- `server.js:238-336` (early check) âœ… Keep this
- `retrieve.js:235-251` (inside retrieveRelevantChunks) âŒ Remove this

**Recommendation**: Remove the check in `retrieve.js` since early cache check is more efficient.

See [BACKEND_RECOMMENDATIONS.md](./BACKEND_RECOMMENDATIONS.md) for details.

## API Support

### GPT-5 Models
- **Generation**: GPT-5-mini (Responses API)
- **Re-ranking**: GPT-5-nano (Responses API)
- **Reasoning effort**: Minimal (optimized for speed)
- **Verbosity**: Low (concise responses)

### Streaming
- Endpoint: `/chat/stream`
- Protocol: Server-Sent Events (SSE)
- Hybrid: Serves cached responses as JSON, streams new responses

## Monitoring & Observability

### Key Metrics Tracked
- Response time breakdown (cache, retrieval, generation)
- Cache hit rate
- Re-ranking trigger rate
- Clarification request rate
- Feedback sentiment

### Performance Logs
```javascript
console.log('ğŸ“Š Performance Breakdown:', {
  retrieval: `${retrievalTime}ms`,
  generation: `${generationTime}ms`,
  total: `${responseTime}ms`,
  rerankingApplied: true/false,
  cacheHit: true/false
});
```

## Future Enhancements

1. **Semantic Caching**: Cache similar queries, not just exact matches
2. **Progressive Retrieval**: Start with top-3, fetch more if needed
3. **Hybrid Search**: Combine vector + keyword search
4. **Query Classification**: Route to specialized pipelines

---

*Last Updated: October 12, 2025*
*Reflects production architecture as of October 2025*
*Based on code analysis of intranetChatbot repository*
